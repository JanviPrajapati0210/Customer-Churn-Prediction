ğŸ“‰ Customer Churn Prediction Project

ğŸ“Œ Objective :
Build an end-to-end Customer Churn Prediction system to identify customers who are likely to leave a service, using machine learning and data-driven insights.

This project focuses on:
â€¢	Understanding customer behavior through EDA
â€¢	Creating meaningful features to capture churn patterns
â€¢	Training and comparing multiple ML models
â€¢	Optimizing decision thresholds for business impact

ğŸ“Š Dataset

â€¢	Dataset used: data/raw/Customer churn data.csv
â€¢	The dataset contains 10,000 customer records with the following information:
â€¢	Customer demographics (Age, Gender, Country)
â€¢	Financial attributes (Credit score, Balance, Salary)
â€¢	Product & engagement data (Tenure, Products, Credit card, Active member)
â€¢	Target variable: churn
â€¢	0 â†’ Customer stays
â€¢	1 â†’ Customer churns

ğŸ› ï¸ Libraries Used

1.	Python
2.	NumPy
3.	Pandas
4.	Matplotlib
5.	Seaborn
6.	Scikit-learn
7.	Jupyter Notebook

ğŸ“ˆ Exploratory Data Analysis (EDA)

EDA was performed to understand how different factors influence customer churn.
Key visualizations include:
ïƒ˜	Distribution of numerical features by churn status
ïƒ˜	Pairwise relationships between financial variables
ïƒ˜	Age distribution and tenure analysis
ïƒ˜	Gender, country, and product-based churn comparison
ïƒ˜	Correlation heatmap
ïƒ˜	Balance vs number of products analysis
ïƒ˜	All visualizations are saved in the reports/ folder.

ğŸ§  Feature Engineering

Custom features were created to better capture customer behavior:
1.	Balance per product
2.	Zero balance indicator
3.	Salary to balance ratio
4.	Customer engagement flag
5.	Age groups
6.	Tenure groups
7.	Product usage category
8.	Credit score bands
These features improved both model performance and interpretability.

âš™ï¸ Data Preprocessing

A clean and reusable preprocessing pipeline was built using ColumnTransformer:
â€¢	Numerical Features
â€¢	Missing value imputation (median)
â€¢	Standard scaling
â€¢	Categorical Features
â€¢	Missing value imputation (most frequent)
â€¢	One-hot encoding

ğŸ¤– Models Trained

The following models were trained and evaluated using the same preprocessing pipeline:

1.	Logistic Regression
2.	Random Forest Classifier
3.	Gradient Boosting Classifier

ğŸ¯ Model Evaluation Strategy

Models were evaluated using:

â€¢	Accuracy
â€¢	Precision
â€¢	Recall
â€¢	F1-Score
â€¢	ROC-AUC
â€¢	Confusion Matrix

Since churn prediction is a business-critical problem, recall for churners was prioritized.

ğŸšï¸ Threshold Optimization

Instead of relying on the default 0.5 probability threshold, multiple thresholds were tested:

ï‚§	0.50
ï‚§	0.40
ï‚§	0.35
ï‚§	0.30 (best trade-off)

Lowering the threshold significantly improved recall, ensuring fewer churners were missed.

ğŸ† Final Model & Results
âœ… Final Model: Gradient Boosting Classifier
Optimal Threshold: 0.30

Performance Highlights:

ïƒ¼	High recall for churned customers
ïƒ¼	Balanced precision and F1-score
ïƒ¼	ROC-AUC â‰ˆ 0.87, indicating strong class separation

This model provides the best balance between business needs and predictive performance.

ğŸ“Œ Key Insights (Summary)

â€¢	Customers with low tenure and low engagement are more likely to churn
â€¢	Zero-balance customers show a higher churn tendency
â€¢	Customers with multiple products are generally more stable
â€¢	Credit score and age play an important role in churn behavior
â€¢	Adjusting the probability threshold improves business-oriented outcomes

ğŸš€ Future Improvements

â€¢	Hyperparameter tuning using GridSearchCV
â€¢	Advanced models like XGBoost or LightGBM
â€¢	Model explainability using SHAP
â€¢	Deployment as a REST API using Flask/FastAPI

